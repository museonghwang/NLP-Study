{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbyOiS3y8Lpb"
      },
      "source": [
        "# 1. 글자 단위 RNN 언어 모델(Char RNNLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cwFLoSSx5KlO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import urllib.request\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6-PUxpHa5NpX"
      },
      "outputs": [],
      "source": [
        "# urllib.request.urlretrieve(\"http://www.gutenberg.org/files/11/11-0.txt\", filename=\"11-0.txt\")\n",
        "f = open('./dataset/11-0.txt', 'rb')\n",
        "sentences = []\n",
        "for sentence in f: # 데이터를 한 줄씩 읽는다.\n",
        "    sentence = sentence.strip() # strip()을 통해 \\r, \\n을 제거한다.\n",
        "    sentence = sentence.lower() # 소문자화.\n",
        "    sentence = sentence.decode('ascii', 'ignore') # \\xe2\\x80\\x99 등과 같은 바이트 열 제거\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKP7Zi2f5OcU",
        "outputId": "e2a670ca-9ce1-474e-9585-63f19aced020"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of alices adventures in wonderland, by lewis carroll',\n",
              " 'this ebook is for the use of anyone anywhere in the united states and',\n",
              " 'most other parts of the world at no cost and with almost no restrictions',\n",
              " 'whatsoever. you may copy it, give it away or re-use it under the terms',\n",
              " 'of the project gutenberg license included with this ebook or online at']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVccL5NL54i7",
        "outputId": "74d760eb-6b0c-41bf-b123-dbce346638a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문자열의 길이 또는 총 글자의 개수: 159484\n"
          ]
        }
      ],
      "source": [
        "total_data = ' '.join(sentences)\n",
        "print('문자열의 길이 또는 총 글자의 개수: %d' % len(total_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBoyjNpx6Zfi",
        "outputId": "43c1c51e-96e2-4ce5-b9b1-87aca44062c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the project gutenberg ebook of alices adventures in wonderland, by lewis carroll this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with\n"
          ]
        }
      ],
      "source": [
        "print(total_data[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-Ttlxpl6abZ",
        "outputId": "7cfeb1cd-c54d-44ff-ab42-d517cf7e46e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "글자 집합의 크기 : 56\n"
          ]
        }
      ],
      "source": [
        "char_vocab = sorted(list(set(total_data)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('글자 집합의 크기 : {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELPHZ-6W6dor",
        "outputId": "f2071f65-3dad-440e-ed09-63fcb0a21dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 0, '!': 1, '\"': 2, '#': 3, '$': 4, '%': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, ',': 10, '-': 11, '.': 12, '/': 13, '0': 14, '1': 15, '2': 16, '3': 17, '4': 18, '5': 19, '6': 20, '7': 21, '8': 22, '9': 23, ':': 24, ';': 25, '?': 26, '[': 27, ']': 28, '_': 29, 'a': 30, 'b': 31, 'c': 32, 'd': 33, 'e': 34, 'f': 35, 'g': 36, 'h': 37, 'i': 38, 'j': 39, 'k': 40, 'l': 41, 'm': 42, 'n': 43, 'o': 44, 'p': 45, 'q': 46, 'r': 47, 's': 48, 't': 49, 'u': 50, 'v': 51, 'w': 52, 'x': 53, 'y': 54, 'z': 55}\n"
          ]
        }
      ],
      "source": [
        "char_to_index = dict((char, index) for index, char in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bsi1VHGO6hLz"
      },
      "outputs": [],
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjR9VLzu6jjD",
        "outputId": "b1553c58-1eef-4dd0-d420-1c639fe348b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 샘플의 수 : 2658\n"
          ]
        }
      ],
      "source": [
        "seq_length = 60 # 문장의 길이를 60으로 한다.\n",
        "n_samples = int(np.floor((len(total_data) - 1) / seq_length)) # 문자열을 60등분한다. 그러면 즉, 총 샘플의 개수\n",
        "print ('문장 샘플의 수 : {}'.format(n_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zdy4iRw56lPL"
      },
      "outputs": [],
      "source": [
        "train_X = []\n",
        "train_y = []\n",
        "\n",
        "for i in range(n_samples):\n",
        "    # 0:60 -> 60:120 -> 120:180로 loop를 돌면서 문장 샘플을 1개씩 픽한다.\n",
        "    X_sample = total_data[i * seq_length: (i + 1) * seq_length]\n",
        "\n",
        "    # 정수 인코딩\n",
        "    X_encoded = [char_to_index[c] for c in X_sample]\n",
        "    train_X.append(X_encoded)\n",
        "\n",
        "    # 오른쪽으로 1칸 쉬프트\n",
        "    y_sample = total_data[i * seq_length + 1: (i + 1) * seq_length + 1]\n",
        "    y_encoded = [char_to_index[c] for c in y_sample]\n",
        "    train_y.append(y_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WShbQjlX64Iz",
        "outputId": "cf998df5-ded5-4556-a771-111fa65d1cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X 데이터의 첫번째 샘플 : [49, 37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30]\n",
            "y 데이터의 첫번째 샘플 : [37, 34, 0, 45, 47, 44, 39, 34, 32, 49, 0, 36, 50, 49, 34, 43, 31, 34, 47, 36, 0, 34, 31, 44, 44, 40, 0, 44, 35, 0, 30, 41, 38, 32, 34, 48, 0, 30, 33, 51, 34, 43, 49, 50, 47, 34, 48, 0, 38, 43, 0, 52, 44, 43, 33, 34, 47, 41, 30, 43]\n",
            "--------------------------------------------------\n",
            "X 데이터의 첫번째 샘플 디코딩 : ['t', 'h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a']\n",
            "y 데이터의 첫번째 샘플 디코딩 : ['h', 'e', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'g', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', 'e', 'b', 'o', 'o', 'k', ' ', 'o', 'f', ' ', 'a', 'l', 'i', 'c', 'e', 's', ' ', 'a', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'i', 'n', ' ', 'w', 'o', 'n', 'd', 'e', 'r', 'l', 'a', 'n']\n"
          ]
        }
      ],
      "source": [
        "print('X 데이터의 첫번째 샘플 :',train_X[0])\n",
        "print('y 데이터의 첫번째 샘플 :',train_y[0])\n",
        "print('-'*50)\n",
        "print('X 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_X[0]])\n",
        "print('y 데이터의 첫번째 샘플 디코딩 :',[index_to_char[i] for i in train_y[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jsksKE17L6y",
        "outputId": "e57fa9ef-d8c7-486e-d21b-13aec531b7d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[43, 33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54]\n"
          ]
        }
      ],
      "source": [
        "print(train_X[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0487oJ67NZl",
        "outputId": "6dc7bff8-c18f-41ba-d83d-ed34cc3e66a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[33, 10, 0, 31, 54, 0, 41, 34, 52, 38, 48, 0, 32, 30, 47, 47, 44, 41, 41, 0, 49, 37, 38, 48, 0, 34, 31, 44, 44, 40, 0, 38, 48, 0, 35, 44, 47, 0, 49, 37, 34, 0, 50, 48, 34, 0, 44, 35, 0, 30, 43, 54, 44, 43, 34, 0, 30, 43, 54, 52]\n"
          ]
        }
      ],
      "source": [
        "print(train_y[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TnGMrM1k7OKi"
      },
      "outputs": [],
      "source": [
        "train_X = to_categorical(train_X)\n",
        "train_y = to_categorical(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipt5Vbf_7bCD",
        "outputId": "6f7f9fcc-835b-4bd7-f517-ce89d1c85d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_X의 크기(shape) : (2658, 60, 56)\n",
            "train_y의 크기(shape) : (2658, 60, 56)\n"
          ]
        }
      ],
      "source": [
        "print('train_X의 크기(shape) : {}'.format(train_X.shape)) # 원-핫 인코딩\n",
        "print('train_y의 크기(shape) : {}'.format(train_y.shape)) # 원-핫 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHsf4lJm7cAj",
        "outputId": "f49b8558-ac79-49ed-a2ca-b87c1637058d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, None, 256)         320512    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, None, 256)         525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, None, 56)         14392     \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 860,216\n",
            "Trainable params: 860,216\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, TimeDistributed\n",
        "\n",
        "hidden_units = 256\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(None, train_X.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(hidden_units, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(vocab_size, activation='softmax')))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlPjFFJq7jcr",
        "outputId": "de0e9853-d981-4e9e-9cd7-e0dfb72b6b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/80\n",
            "84/84 - 4s - loss: 3.0847 - accuracy: 0.1803 - 4s/epoch - 42ms/step\n",
            "Epoch 2/80\n",
            "84/84 - 2s - loss: 2.7986 - accuracy: 0.2300 - 2s/epoch - 18ms/step\n",
            "Epoch 3/80\n",
            "84/84 - 1s - loss: 2.4115 - accuracy: 0.3259 - 1s/epoch - 17ms/step\n",
            "Epoch 4/80\n",
            "84/84 - 1s - loss: 2.2432 - accuracy: 0.3623 - 1s/epoch - 18ms/step\n",
            "Epoch 5/80\n",
            "84/84 - 2s - loss: 2.1505 - accuracy: 0.3858 - 2s/epoch - 19ms/step\n",
            "Epoch 6/80\n",
            "84/84 - 1s - loss: 2.0504 - accuracy: 0.4098 - 1s/epoch - 16ms/step\n",
            "Epoch 7/80\n",
            "84/84 - 2s - loss: 1.9774 - accuracy: 0.4285 - 2s/epoch - 19ms/step\n",
            "Epoch 8/80\n",
            "84/84 - 1s - loss: 1.9128 - accuracy: 0.4461 - 1s/epoch - 17ms/step\n",
            "Epoch 9/80\n",
            "84/84 - 1s - loss: 1.8552 - accuracy: 0.4632 - 1s/epoch - 17ms/step\n",
            "Epoch 10/80\n",
            "84/84 - 2s - loss: 1.8033 - accuracy: 0.4771 - 2s/epoch - 18ms/step\n",
            "Epoch 11/80\n",
            "84/84 - 2s - loss: 1.7574 - accuracy: 0.4893 - 2s/epoch - 19ms/step\n",
            "Epoch 12/80\n",
            "84/84 - 1s - loss: 1.7140 - accuracy: 0.5016 - 1s/epoch - 17ms/step\n",
            "Epoch 13/80\n",
            "84/84 - 2s - loss: 1.6730 - accuracy: 0.5115 - 2s/epoch - 18ms/step\n",
            "Epoch 14/80\n",
            "84/84 - 2s - loss: 1.6376 - accuracy: 0.5217 - 2s/epoch - 20ms/step\n",
            "Epoch 15/80\n",
            "84/84 - 1s - loss: 1.6020 - accuracy: 0.5306 - 1s/epoch - 18ms/step\n",
            "Epoch 16/80\n",
            "84/84 - 1s - loss: 1.5662 - accuracy: 0.5393 - 1s/epoch - 16ms/step\n",
            "Epoch 17/80\n",
            "84/84 - 1s - loss: 1.5344 - accuracy: 0.5478 - 1s/epoch - 18ms/step\n",
            "Epoch 18/80\n",
            "84/84 - 2s - loss: 1.5048 - accuracy: 0.5555 - 2s/epoch - 19ms/step\n",
            "Epoch 19/80\n",
            "84/84 - 2s - loss: 1.4737 - accuracy: 0.5638 - 2s/epoch - 18ms/step\n",
            "Epoch 20/80\n",
            "84/84 - 2s - loss: 1.4456 - accuracy: 0.5720 - 2s/epoch - 18ms/step\n",
            "Epoch 21/80\n",
            "84/84 - 2s - loss: 1.4137 - accuracy: 0.5807 - 2s/epoch - 19ms/step\n",
            "Epoch 22/80\n",
            "84/84 - 2s - loss: 1.3883 - accuracy: 0.5867 - 2s/epoch - 20ms/step\n",
            "Epoch 23/80\n",
            "84/84 - 1s - loss: 1.3581 - accuracy: 0.5958 - 1s/epoch - 17ms/step\n",
            "Epoch 24/80\n",
            "84/84 - 2s - loss: 1.3318 - accuracy: 0.6037 - 2s/epoch - 18ms/step\n",
            "Epoch 25/80\n",
            "84/84 - 2s - loss: 1.3036 - accuracy: 0.6115 - 2s/epoch - 20ms/step\n",
            "Epoch 26/80\n",
            "84/84 - 2s - loss: 1.2781 - accuracy: 0.6180 - 2s/epoch - 18ms/step\n",
            "Epoch 27/80\n",
            "84/84 - 2s - loss: 1.2532 - accuracy: 0.6250 - 2s/epoch - 18ms/step\n",
            "Epoch 28/80\n",
            "84/84 - 1s - loss: 1.2227 - accuracy: 0.6354 - 1s/epoch - 17ms/step\n",
            "Epoch 29/80\n",
            "84/84 - 2s - loss: 1.1995 - accuracy: 0.6413 - 2s/epoch - 18ms/step\n",
            "Epoch 30/80\n",
            "84/84 - 1s - loss: 1.1759 - accuracy: 0.6483 - 1s/epoch - 17ms/step\n",
            "Epoch 31/80\n",
            "84/84 - 1s - loss: 1.1446 - accuracy: 0.6576 - 1s/epoch - 18ms/step\n",
            "Epoch 32/80\n",
            "84/84 - 2s - loss: 1.1180 - accuracy: 0.6662 - 2s/epoch - 18ms/step\n",
            "Epoch 33/80\n",
            "84/84 - 2s - loss: 1.0913 - accuracy: 0.6736 - 2s/epoch - 18ms/step\n",
            "Epoch 34/80\n",
            "84/84 - 2s - loss: 1.0672 - accuracy: 0.6814 - 2s/epoch - 18ms/step\n",
            "Epoch 35/80\n",
            "84/84 - 2s - loss: 1.0365 - accuracy: 0.6902 - 2s/epoch - 19ms/step\n",
            "Epoch 36/80\n",
            "84/84 - 2s - loss: 1.0128 - accuracy: 0.6972 - 2s/epoch - 19ms/step\n",
            "Epoch 37/80\n",
            "84/84 - 2s - loss: 0.9883 - accuracy: 0.7044 - 2s/epoch - 18ms/step\n",
            "Epoch 38/80\n",
            "84/84 - 1s - loss: 0.9604 - accuracy: 0.7140 - 1s/epoch - 17ms/step\n",
            "Epoch 39/80\n",
            "84/84 - 1s - loss: 0.9325 - accuracy: 0.7212 - 1s/epoch - 16ms/step\n",
            "Epoch 40/80\n",
            "84/84 - 1s - loss: 0.9054 - accuracy: 0.7311 - 1s/epoch - 16ms/step\n",
            "Epoch 41/80\n",
            "84/84 - 1s - loss: 0.8756 - accuracy: 0.7401 - 1s/epoch - 18ms/step\n",
            "Epoch 42/80\n",
            "84/84 - 2s - loss: 0.8502 - accuracy: 0.7478 - 2s/epoch - 19ms/step\n",
            "Epoch 43/80\n",
            "84/84 - 1s - loss: 0.8237 - accuracy: 0.7573 - 1s/epoch - 18ms/step\n",
            "Epoch 44/80\n",
            "84/84 - 1s - loss: 0.8129 - accuracy: 0.7586 - 1s/epoch - 17ms/step\n",
            "Epoch 45/80\n",
            "84/84 - 2s - loss: 0.7882 - accuracy: 0.7663 - 2s/epoch - 20ms/step\n",
            "Epoch 46/80\n",
            "84/84 - 2s - loss: 0.7504 - accuracy: 0.7802 - 2s/epoch - 19ms/step\n",
            "Epoch 47/80\n",
            "84/84 - 1s - loss: 0.7278 - accuracy: 0.7870 - 1s/epoch - 17ms/step\n",
            "Epoch 48/80\n",
            "84/84 - 1s - loss: 0.7066 - accuracy: 0.7930 - 1s/epoch - 18ms/step\n",
            "Epoch 49/80\n",
            "84/84 - 2s - loss: 0.6883 - accuracy: 0.7991 - 2s/epoch - 21ms/step\n",
            "Epoch 50/80\n",
            "84/84 - 2s - loss: 0.6639 - accuracy: 0.8065 - 2s/epoch - 19ms/step\n",
            "Epoch 51/80\n",
            "84/84 - 1s - loss: 0.6399 - accuracy: 0.8147 - 1s/epoch - 17ms/step\n",
            "Epoch 52/80\n",
            "84/84 - 2s - loss: 0.6544 - accuracy: 0.8054 - 2s/epoch - 18ms/step\n",
            "Epoch 53/80\n",
            "84/84 - 2s - loss: 0.5999 - accuracy: 0.8283 - 2s/epoch - 21ms/step\n",
            "Epoch 54/80\n",
            "84/84 - 1s - loss: 0.5750 - accuracy: 0.8359 - 1s/epoch - 18ms/step\n",
            "Epoch 55/80\n",
            "84/84 - 1s - loss: 0.5480 - accuracy: 0.8454 - 1s/epoch - 16ms/step\n",
            "Epoch 56/80\n",
            "84/84 - 2s - loss: 0.5335 - accuracy: 0.8499 - 2s/epoch - 20ms/step\n",
            "Epoch 57/80\n",
            "84/84 - 1s - loss: 0.5136 - accuracy: 0.8556 - 1s/epoch - 17ms/step\n",
            "Epoch 58/80\n",
            "84/84 - 1s - loss: 0.4979 - accuracy: 0.8603 - 1s/epoch - 17ms/step\n",
            "Epoch 59/80\n",
            "84/84 - 1s - loss: 0.4829 - accuracy: 0.8650 - 1s/epoch - 17ms/step\n",
            "Epoch 60/80\n",
            "84/84 - 2s - loss: 0.4647 - accuracy: 0.8715 - 2s/epoch - 20ms/step\n",
            "Epoch 61/80\n",
            "84/84 - 1s - loss: 0.4429 - accuracy: 0.8785 - 1s/epoch - 18ms/step\n",
            "Epoch 62/80\n",
            "84/84 - 2s - loss: 0.4304 - accuracy: 0.8826 - 2s/epoch - 18ms/step\n",
            "Epoch 63/80\n",
            "84/84 - 2s - loss: 0.4048 - accuracy: 0.8923 - 2s/epoch - 19ms/step\n",
            "Epoch 64/80\n",
            "84/84 - 1s - loss: 0.3948 - accuracy: 0.8936 - 1s/epoch - 17ms/step\n",
            "Epoch 65/80\n",
            "84/84 - 1s - loss: 0.3762 - accuracy: 0.9008 - 1s/epoch - 17ms/step\n",
            "Epoch 66/80\n",
            "84/84 - 2s - loss: 0.3650 - accuracy: 0.9031 - 2s/epoch - 18ms/step\n",
            "Epoch 67/80\n",
            "84/84 - 2s - loss: 0.3443 - accuracy: 0.9113 - 2s/epoch - 20ms/step\n",
            "Epoch 68/80\n",
            "84/84 - 2s - loss: 0.3426 - accuracy: 0.9101 - 2s/epoch - 18ms/step\n",
            "Epoch 69/80\n",
            "84/84 - 1s - loss: 0.3213 - accuracy: 0.9180 - 1s/epoch - 18ms/step\n",
            "Epoch 70/80\n",
            "84/84 - 2s - loss: 0.3063 - accuracy: 0.9225 - 2s/epoch - 19ms/step\n",
            "Epoch 71/80\n",
            "84/84 - 1s - loss: 0.2970 - accuracy: 0.9254 - 1s/epoch - 17ms/step\n",
            "Epoch 72/80\n",
            "84/84 - 2s - loss: 0.2846 - accuracy: 0.9292 - 2s/epoch - 18ms/step\n",
            "Epoch 73/80\n",
            "84/84 - 2s - loss: 0.2749 - accuracy: 0.9328 - 2s/epoch - 19ms/step\n",
            "Epoch 74/80\n",
            "84/84 - 2s - loss: 0.2655 - accuracy: 0.9341 - 2s/epoch - 20ms/step\n",
            "Epoch 75/80\n",
            "84/84 - 1s - loss: 0.2629 - accuracy: 0.9347 - 1s/epoch - 18ms/step\n",
            "Epoch 76/80\n",
            "84/84 - 2s - loss: 0.2526 - accuracy: 0.9374 - 2s/epoch - 18ms/step\n",
            "Epoch 77/80\n",
            "84/84 - 2s - loss: 0.2459 - accuracy: 0.9390 - 2s/epoch - 18ms/step\n",
            "Epoch 78/80\n",
            "84/84 - 2s - loss: 0.2389 - accuracy: 0.9415 - 2s/epoch - 19ms/step\n",
            "Epoch 79/80\n",
            "84/84 - 1s - loss: 0.2193 - accuracy: 0.9477 - 1s/epoch - 15ms/step\n",
            "Epoch 80/80\n",
            "84/84 - 2s - loss: 0.2166 - accuracy: 0.9480 - 2s/epoch - 19ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbcf80ce280>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X, train_y, epochs=80, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8lX31r2y7lbY"
      },
      "outputs": [],
      "source": [
        "def sentence_generation(model, length):\n",
        "    # 글자에 대한 랜덤 인덱스 생성\n",
        "    ix = [np.random.randint(vocab_size)]\n",
        "\n",
        "    # 랜덤 익덱스로부터 글자 생성\n",
        "    y_char = [index_to_char[ix[-1]]]\n",
        "    print(ix[-1],'번 글자',y_char[-1],'로 예측을 시작!')\n",
        "\n",
        "    # (1, length, 55) 크기의 X 생성. 즉, LSTM의 입력 시퀀스 생성\n",
        "    X = np.zeros((1, length, vocab_size))\n",
        "    \n",
        "    for i in range(length):\n",
        "        # X[0][i][예측한 글자의 인덱스] = 1, 즉, 예측 글자를 다음 입력 시퀀스에 추가\n",
        "        X[0][i][ix[-1]] = 1\n",
        "        print(index_to_char[ix[-1]], end=\"\")\n",
        "        ix = np.argmax(model.predict(X[:, :i+1, :])[0], 1)\n",
        "        y_char.append(index_to_char[ix[-1]])\n",
        "    return ('').join(y_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgSKvzAE7zaD",
        "outputId": "cd27625e-3921-4e8d-d1f8-57c1bc66d912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11 번 글자 - 로 예측을 시작!\n",
            "1/1 [==============================] - 0s 281ms/step\n",
            "1/1 [==============================] - 0s 270ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-box, or they would die. the trial cannot proceed, said the king, looking rather hears. thats notting\n"
          ]
        }
      ],
      "source": [
        "result = sentence_generation(model, 100)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjb93Az58QIO"
      },
      "source": [
        "# 2. 글자 단위 RNN(Char RNN)으로 텍스트 생성하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMMHgEPr8R1n"
      },
      "source": [
        "이번에는 다 대 일(many-to-one) 구조의 RNN을 글자 단위로 학습시키고, 텍스트 생성을 해보겠습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "D7PwoCcN7-rL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2D_GYxw28S6c"
      },
      "outputs": [],
      "source": [
        "raw_text = '''\n",
        "I get on with life as a programmer,\n",
        "I like to contemplate beer.\n",
        "But when I start to daydream,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "Do I love wine more than beer?\n",
        "\n",
        "I like to use words about beer.\n",
        "But when I stop my talking,\n",
        "My mind turns straight to wine.\n",
        "\n",
        "I hate bugs and errors.\n",
        "But I just think back to wine,\n",
        "And I'm happy once again.\n",
        "\n",
        "I like to hang out with programming and deep learning.\n",
        "But when left alone,\n",
        "My mind turns straight to wine.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "823KE-Oh8Uoa",
        "outputId": "26f69b62-dab2-459d-ceb8-124206afe020"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I get on with life as a programmer, I like to contemplate beer. But when I start to daydream, My mind turns straight to wine. Do I love wine more than beer? I like to use words about beer. But when I stop my talking, My mind turns straight to wine. I hate bugs and errors. But I just think back to wine, And I'm happy once again. I like to hang out with programming and deep learning. But when left alone, My mind turns straight to wine.\n"
          ]
        }
      ],
      "source": [
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "print(raw_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfxe6-xO8WIE",
        "outputId": "ba12d8b3-b6eb-46af-a06a-6eda7f34a8b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ', \"'\", ',', '.', '?', 'A', 'B', 'D', 'I', 'M', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n"
          ]
        }
      ],
      "source": [
        "# 중복을 제거한 글자 집합 생성\n",
        "char_vocab = sorted(list(set(raw_text)))\n",
        "print(char_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVf-hgG18Zv0",
        "outputId": "95185a97-d74c-447e-fdba-2379b2fa8eaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "글자 집합의 크기 : 33\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(char_vocab)\n",
        "print ('글자 집합의 크기 : {}'.format(vocab_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA1ctt5t8eRa",
        "outputId": "8b758eb7-0dd0-473d-b1c2-4257461b1b47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 0, \"'\": 1, ',': 2, '.': 3, '?': 4, 'A': 5, 'B': 6, 'D': 7, 'I': 8, 'M': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'r': 26, 's': 27, 't': 28, 'u': 29, 'v': 30, 'w': 31, 'y': 32}\n"
          ]
        }
      ],
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 글자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiZW1obf8eaE",
        "outputId": "6d0bfa55-1fe6-458b-e194-4904f7310148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 훈련 샘플의 수: 426\n"
          ]
        }
      ],
      "source": [
        "length = 11\n",
        "sequences = []\n",
        "for i in range(length, len(raw_text)):\n",
        "    seq = raw_text[i-length:i] # 길이 11의 문자열을 지속적으로 만든다.\n",
        "    sequences.append(seq)\n",
        "print('총 훈련 샘플의 수: %d' % len(sequences))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ait_ciP98hD8",
        "outputId": "8f4d7976-6265-499e-cb96-ac24ad4e50e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I get on wi',\n",
              " ' get on wit',\n",
              " 'get on with',\n",
              " 'et on with ',\n",
              " 't on with l',\n",
              " ' on with li',\n",
              " 'on with lif',\n",
              " 'n with life',\n",
              " ' with life ',\n",
              " 'with life a']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bhiQ6Uq58iD6"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = []\n",
        "for sequence in sequences: # 전체 데이터에서 문장 샘플을 1개씩 꺼낸다.\n",
        "    encoded_sequence = [char_to_index[char] for char in sequence] # 문장 샘플에서 각 글자에 대해서 정수 인코딩을 수행.\n",
        "    encoded_sequences.append(encoded_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz1bV5D_1rWu",
        "outputId": "cabc0304-5bc7-4ab0-eabc-64294262a5b4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[8, 0, 16, 14, 28, 0, 24, 23, 0, 31, 18],\n",
              " [0, 16, 14, 28, 0, 24, 23, 0, 31, 18, 28],\n",
              " [16, 14, 28, 0, 24, 23, 0, 31, 18, 28, 17],\n",
              " [14, 28, 0, 24, 23, 0, 31, 18, 28, 17, 0],\n",
              " [28, 0, 24, 23, 0, 31, 18, 28, 17, 0, 21]]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_sequences[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3f5fQ9Wc1w_t"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = np.array(encoded_sequences)\n",
        "X_data = encoded_sequences[:,:-1]\n",
        "\n",
        "# 맨 마지막 위치의 글자를 분리\n",
        "y_data = encoded_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s--PreVs1-Sd",
        "outputId": "18f3dd0a-8a70-4eff-93fa-add9c6704613"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 8  0 16 14 28  0 24 23  0 31]\n",
            " [ 0 16 14 28  0 24 23  0 31 18]\n",
            " [16 14 28  0 24 23  0 31 18 28]\n",
            " [14 28  0 24 23  0 31 18 28 17]\n",
            " [28  0 24 23  0 31 18 28 17  0]]\n",
            "[18 28 17  0 21]\n"
          ]
        }
      ],
      "source": [
        "print(X_data[:5])\n",
        "print(y_data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "q_CQgpkw2Hc4"
      },
      "outputs": [],
      "source": [
        "# 원-핫 인코딩\n",
        "X_data_one_hot = [to_categorical(encoded, num_classes=vocab_size) for encoded in X_data]\n",
        "X_data_one_hot = np.array(X_data_one_hot)\n",
        "y_data_one_hot = to_categorical(y_data, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5NEsRym2bBo",
        "outputId": "7e8f9900-fea1-43aa-8dc5-5097c398fa6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(426, 10, 33)\n"
          ]
        }
      ],
      "source": [
        "print(X_data_one_hot.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "8CqLyAsG2dP9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LiqNlvRu2egu"
      },
      "outputs": [],
      "source": [
        "hidden_units = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(hidden_units, input_shape=(X_data_one_hot.shape[1], X_data_one_hot.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymJjEjd42i_l",
        "outputId": "61ecaef0-463d-4304-8c01-67b7c328f461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "14/14 - 1s - loss: 3.4721 - accuracy: 0.1315 - 630ms/epoch - 45ms/step\n",
            "Epoch 2/100\n",
            "14/14 - 0s - loss: 3.3817 - accuracy: 0.1995 - 82ms/epoch - 6ms/step\n",
            "Epoch 3/100\n",
            "14/14 - 0s - loss: 3.1340 - accuracy: 0.1972 - 94ms/epoch - 7ms/step\n",
            "Epoch 4/100\n",
            "14/14 - 0s - loss: 3.0050 - accuracy: 0.1972 - 96ms/epoch - 7ms/step\n",
            "Epoch 5/100\n",
            "14/14 - 0s - loss: 2.9640 - accuracy: 0.1972 - 79ms/epoch - 6ms/step\n",
            "Epoch 6/100\n",
            "14/14 - 0s - loss: 2.9365 - accuracy: 0.1972 - 68ms/epoch - 5ms/step\n",
            "Epoch 7/100\n",
            "14/14 - 0s - loss: 2.9255 - accuracy: 0.1972 - 89ms/epoch - 6ms/step\n",
            "Epoch 8/100\n",
            "14/14 - 0s - loss: 2.9074 - accuracy: 0.1972 - 92ms/epoch - 7ms/step\n",
            "Epoch 9/100\n",
            "14/14 - 0s - loss: 2.8891 - accuracy: 0.1972 - 97ms/epoch - 7ms/step\n",
            "Epoch 10/100\n",
            "14/14 - 0s - loss: 2.8689 - accuracy: 0.1972 - 89ms/epoch - 6ms/step\n",
            "Epoch 11/100\n",
            "14/14 - 0s - loss: 2.8436 - accuracy: 0.1972 - 97ms/epoch - 7ms/step\n",
            "Epoch 12/100\n",
            "14/14 - 0s - loss: 2.8213 - accuracy: 0.2019 - 83ms/epoch - 6ms/step\n",
            "Epoch 13/100\n",
            "14/14 - 0s - loss: 2.7893 - accuracy: 0.1972 - 41ms/epoch - 3ms/step\n",
            "Epoch 14/100\n",
            "14/14 - 0s - loss: 2.7499 - accuracy: 0.2089 - 49ms/epoch - 4ms/step\n",
            "Epoch 15/100\n",
            "14/14 - 0s - loss: 2.6982 - accuracy: 0.2042 - 99ms/epoch - 7ms/step\n",
            "Epoch 16/100\n",
            "14/14 - 0s - loss: 2.6639 - accuracy: 0.2770 - 93ms/epoch - 7ms/step\n",
            "Epoch 17/100\n",
            "14/14 - 0s - loss: 2.5957 - accuracy: 0.2254 - 88ms/epoch - 6ms/step\n",
            "Epoch 18/100\n",
            "14/14 - 0s - loss: 2.5512 - accuracy: 0.2723 - 97ms/epoch - 7ms/step\n",
            "Epoch 19/100\n",
            "14/14 - 0s - loss: 2.5016 - accuracy: 0.2746 - 31ms/epoch - 2ms/step\n",
            "Epoch 20/100\n",
            "14/14 - 0s - loss: 2.4485 - accuracy: 0.2793 - 52ms/epoch - 4ms/step\n",
            "Epoch 21/100\n",
            "14/14 - 0s - loss: 2.3940 - accuracy: 0.2700 - 42ms/epoch - 3ms/step\n",
            "Epoch 22/100\n",
            "14/14 - 0s - loss: 2.3614 - accuracy: 0.3099 - 31ms/epoch - 2ms/step\n",
            "Epoch 23/100\n",
            "14/14 - 0s - loss: 2.3214 - accuracy: 0.3380 - 84ms/epoch - 6ms/step\n",
            "Epoch 24/100\n",
            "14/14 - 0s - loss: 2.2850 - accuracy: 0.3239 - 95ms/epoch - 7ms/step\n",
            "Epoch 25/100\n",
            "14/14 - 0s - loss: 2.2217 - accuracy: 0.3545 - 93ms/epoch - 7ms/step\n",
            "Epoch 26/100\n",
            "14/14 - 0s - loss: 2.1746 - accuracy: 0.3709 - 98ms/epoch - 7ms/step\n",
            "Epoch 27/100\n",
            "14/14 - 0s - loss: 2.1389 - accuracy: 0.4014 - 92ms/epoch - 7ms/step\n",
            "Epoch 28/100\n",
            "14/14 - 0s - loss: 2.0845 - accuracy: 0.3991 - 93ms/epoch - 7ms/step\n",
            "Epoch 29/100\n",
            "14/14 - 0s - loss: 2.0507 - accuracy: 0.4202 - 93ms/epoch - 7ms/step\n",
            "Epoch 30/100\n",
            "14/14 - 0s - loss: 2.0066 - accuracy: 0.4343 - 93ms/epoch - 7ms/step\n",
            "Epoch 31/100\n",
            "14/14 - 0s - loss: 1.9688 - accuracy: 0.4249 - 87ms/epoch - 6ms/step\n",
            "Epoch 32/100\n",
            "14/14 - 0s - loss: 1.9369 - accuracy: 0.4765 - 93ms/epoch - 7ms/step\n",
            "Epoch 33/100\n",
            "14/14 - 0s - loss: 1.8935 - accuracy: 0.4718 - 90ms/epoch - 6ms/step\n",
            "Epoch 34/100\n",
            "14/14 - 0s - loss: 1.8534 - accuracy: 0.4977 - 86ms/epoch - 6ms/step\n",
            "Epoch 35/100\n",
            "14/14 - 0s - loss: 1.8096 - accuracy: 0.5329 - 92ms/epoch - 7ms/step\n",
            "Epoch 36/100\n",
            "14/14 - 0s - loss: 1.7720 - accuracy: 0.5305 - 91ms/epoch - 6ms/step\n",
            "Epoch 37/100\n",
            "14/14 - 0s - loss: 1.7280 - accuracy: 0.5469 - 95ms/epoch - 7ms/step\n",
            "Epoch 38/100\n",
            "14/14 - 0s - loss: 1.6941 - accuracy: 0.5516 - 92ms/epoch - 7ms/step\n",
            "Epoch 39/100\n",
            "14/14 - 0s - loss: 1.6612 - accuracy: 0.5587 - 98ms/epoch - 7ms/step\n",
            "Epoch 40/100\n",
            "14/14 - 0s - loss: 1.6194 - accuracy: 0.5775 - 98ms/epoch - 7ms/step\n",
            "Epoch 41/100\n",
            "14/14 - 0s - loss: 1.6081 - accuracy: 0.5915 - 91ms/epoch - 6ms/step\n",
            "Epoch 42/100\n",
            "14/14 - 0s - loss: 1.5531 - accuracy: 0.5775 - 97ms/epoch - 7ms/step\n",
            "Epoch 43/100\n",
            "14/14 - 0s - loss: 1.5071 - accuracy: 0.5915 - 68ms/epoch - 5ms/step\n",
            "Epoch 44/100\n",
            "14/14 - 0s - loss: 1.4779 - accuracy: 0.6338 - 24ms/epoch - 2ms/step\n",
            "Epoch 45/100\n",
            "14/14 - 0s - loss: 1.4528 - accuracy: 0.6150 - 26ms/epoch - 2ms/step\n",
            "Epoch 46/100\n",
            "14/14 - 0s - loss: 1.4241 - accuracy: 0.6221 - 24ms/epoch - 2ms/step\n",
            "Epoch 47/100\n",
            "14/14 - 0s - loss: 1.3729 - accuracy: 0.6596 - 44ms/epoch - 3ms/step\n",
            "Epoch 48/100\n",
            "14/14 - 0s - loss: 1.3429 - accuracy: 0.6643 - 85ms/epoch - 6ms/step\n",
            "Epoch 49/100\n",
            "14/14 - 0s - loss: 1.3187 - accuracy: 0.6573 - 88ms/epoch - 6ms/step\n",
            "Epoch 50/100\n",
            "14/14 - 0s - loss: 1.2780 - accuracy: 0.6808 - 81ms/epoch - 6ms/step\n",
            "Epoch 51/100\n",
            "14/14 - 0s - loss: 1.2417 - accuracy: 0.6995 - 91ms/epoch - 7ms/step\n",
            "Epoch 52/100\n",
            "14/14 - 0s - loss: 1.2050 - accuracy: 0.7113 - 97ms/epoch - 7ms/step\n",
            "Epoch 53/100\n",
            "14/14 - 0s - loss: 1.1973 - accuracy: 0.7160 - 90ms/epoch - 6ms/step\n",
            "Epoch 54/100\n",
            "14/14 - 0s - loss: 1.1552 - accuracy: 0.7160 - 83ms/epoch - 6ms/step\n",
            "Epoch 55/100\n",
            "14/14 - 0s - loss: 1.1226 - accuracy: 0.7324 - 76ms/epoch - 5ms/step\n",
            "Epoch 56/100\n",
            "14/14 - 0s - loss: 1.1022 - accuracy: 0.7488 - 87ms/epoch - 6ms/step\n",
            "Epoch 57/100\n",
            "14/14 - 0s - loss: 1.0712 - accuracy: 0.7606 - 87ms/epoch - 6ms/step\n",
            "Epoch 58/100\n",
            "14/14 - 0s - loss: 1.0494 - accuracy: 0.7653 - 90ms/epoch - 6ms/step\n",
            "Epoch 59/100\n",
            "14/14 - 0s - loss: 1.0377 - accuracy: 0.7606 - 94ms/epoch - 7ms/step\n",
            "Epoch 60/100\n",
            "14/14 - 0s - loss: 0.9869 - accuracy: 0.7817 - 88ms/epoch - 6ms/step\n",
            "Epoch 61/100\n",
            "14/14 - 0s - loss: 0.9556 - accuracy: 0.8028 - 94ms/epoch - 7ms/step\n",
            "Epoch 62/100\n",
            "14/14 - 0s - loss: 0.9299 - accuracy: 0.8005 - 91ms/epoch - 7ms/step\n",
            "Epoch 63/100\n",
            "14/14 - 0s - loss: 0.9020 - accuracy: 0.8099 - 101ms/epoch - 7ms/step\n",
            "Epoch 64/100\n",
            "14/14 - 0s - loss: 0.8730 - accuracy: 0.8333 - 84ms/epoch - 6ms/step\n",
            "Epoch 65/100\n",
            "14/14 - 0s - loss: 0.8545 - accuracy: 0.8286 - 96ms/epoch - 7ms/step\n",
            "Epoch 66/100\n",
            "14/14 - 0s - loss: 0.8332 - accuracy: 0.8263 - 88ms/epoch - 6ms/step\n",
            "Epoch 67/100\n",
            "14/14 - 0s - loss: 0.8209 - accuracy: 0.8333 - 87ms/epoch - 6ms/step\n",
            "Epoch 68/100\n",
            "14/14 - 0s - loss: 0.7982 - accuracy: 0.8380 - 88ms/epoch - 6ms/step\n",
            "Epoch 69/100\n",
            "14/14 - 0s - loss: 0.7686 - accuracy: 0.8427 - 55ms/epoch - 4ms/step\n",
            "Epoch 70/100\n",
            "14/14 - 0s - loss: 0.7336 - accuracy: 0.8545 - 47ms/epoch - 3ms/step\n",
            "Epoch 71/100\n",
            "14/14 - 0s - loss: 0.7205 - accuracy: 0.8545 - 35ms/epoch - 2ms/step\n",
            "Epoch 72/100\n",
            "14/14 - 0s - loss: 0.7051 - accuracy: 0.8709 - 27ms/epoch - 2ms/step\n",
            "Epoch 73/100\n",
            "14/14 - 0s - loss: 0.6893 - accuracy: 0.8662 - 44ms/epoch - 3ms/step\n",
            "Epoch 74/100\n",
            "14/14 - 0s - loss: 0.6824 - accuracy: 0.8568 - 89ms/epoch - 6ms/step\n",
            "Epoch 75/100\n",
            "14/14 - 0s - loss: 0.6491 - accuracy: 0.8826 - 89ms/epoch - 6ms/step\n",
            "Epoch 76/100\n",
            "14/14 - 0s - loss: 0.6258 - accuracy: 0.8803 - 92ms/epoch - 7ms/step\n",
            "Epoch 77/100\n",
            "14/14 - 0s - loss: 0.6052 - accuracy: 0.8873 - 88ms/epoch - 6ms/step\n",
            "Epoch 78/100\n",
            "14/14 - 0s - loss: 0.5944 - accuracy: 0.9038 - 92ms/epoch - 7ms/step\n",
            "Epoch 79/100\n",
            "14/14 - 0s - loss: 0.5818 - accuracy: 0.8991 - 86ms/epoch - 6ms/step\n",
            "Epoch 80/100\n",
            "14/14 - 0s - loss: 0.5562 - accuracy: 0.9085 - 84ms/epoch - 6ms/step\n",
            "Epoch 81/100\n",
            "14/14 - 0s - loss: 0.5347 - accuracy: 0.9155 - 90ms/epoch - 6ms/step\n",
            "Epoch 82/100\n",
            "14/14 - 0s - loss: 0.5205 - accuracy: 0.9249 - 95ms/epoch - 7ms/step\n",
            "Epoch 83/100\n",
            "14/14 - 0s - loss: 0.5083 - accuracy: 0.9225 - 86ms/epoch - 6ms/step\n",
            "Epoch 84/100\n",
            "14/14 - 0s - loss: 0.4986 - accuracy: 0.9272 - 90ms/epoch - 6ms/step\n",
            "Epoch 85/100\n",
            "14/14 - 0s - loss: 0.4803 - accuracy: 0.9366 - 92ms/epoch - 7ms/step\n",
            "Epoch 86/100\n",
            "14/14 - 0s - loss: 0.4664 - accuracy: 0.9390 - 99ms/epoch - 7ms/step\n",
            "Epoch 87/100\n",
            "14/14 - 0s - loss: 0.4470 - accuracy: 0.9343 - 89ms/epoch - 6ms/step\n",
            "Epoch 88/100\n",
            "14/14 - 0s - loss: 0.4325 - accuracy: 0.9531 - 95ms/epoch - 7ms/step\n",
            "Epoch 89/100\n",
            "14/14 - 0s - loss: 0.4211 - accuracy: 0.9507 - 94ms/epoch - 7ms/step\n",
            "Epoch 90/100\n",
            "14/14 - 0s - loss: 0.4167 - accuracy: 0.9484 - 88ms/epoch - 6ms/step\n",
            "Epoch 91/100\n",
            "14/14 - 0s - loss: 0.4035 - accuracy: 0.9554 - 94ms/epoch - 7ms/step\n",
            "Epoch 92/100\n",
            "14/14 - 0s - loss: 0.3876 - accuracy: 0.9577 - 90ms/epoch - 6ms/step\n",
            "Epoch 93/100\n",
            "14/14 - 0s - loss: 0.3913 - accuracy: 0.9577 - 89ms/epoch - 6ms/step\n",
            "Epoch 94/100\n",
            "14/14 - 0s - loss: 0.3767 - accuracy: 0.9648 - 68ms/epoch - 5ms/step\n",
            "Epoch 95/100\n",
            "14/14 - 0s - loss: 0.3600 - accuracy: 0.9671 - 28ms/epoch - 2ms/step\n",
            "Epoch 96/100\n",
            "14/14 - 0s - loss: 0.3451 - accuracy: 0.9765 - 35ms/epoch - 3ms/step\n",
            "Epoch 97/100\n",
            "14/14 - 0s - loss: 0.3296 - accuracy: 0.9789 - 29ms/epoch - 2ms/step\n",
            "Epoch 98/100\n",
            "14/14 - 0s - loss: 0.3209 - accuracy: 0.9742 - 74ms/epoch - 5ms/step\n",
            "Epoch 99/100\n",
            "14/14 - 0s - loss: 0.3125 - accuracy: 0.9765 - 87ms/epoch - 6ms/step\n",
            "Epoch 100/100\n",
            "14/14 - 0s - loss: 0.3050 - accuracy: 0.9812 - 89ms/epoch - 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbcf8288b20>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_data_one_hot, y_data_one_hot, epochs=100, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "m8hZiN9Y2oH-"
      },
      "outputs": [],
      "source": [
        "def sentence_generation(model, char_to_index, seq_length, seed_text, n):\n",
        "\n",
        "    # 초기 시퀀스\n",
        "    init_text = seed_text\n",
        "    sentence = ''\n",
        "\n",
        "    for _ in range(n):\n",
        "        encoded = [char_to_index[char] for char in seed_text] # 현재 시퀀스에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, padding='pre') # 데이터에 대한 패딩\n",
        "        encoded = to_categorical(encoded, num_classes=len(char_to_index))\n",
        "\n",
        "        # 입력한 X(현재 시퀀스)에 대해서 y를 예측하고 y(예측한 글자)를 result에 저장.\n",
        "        result = model.predict(encoded, verbose=0)\n",
        "        result = np.argmax(result, axis=1)\n",
        "        \n",
        "        for char, index in char_to_index.items():\n",
        "            if index == result:\n",
        "                break\n",
        "\n",
        "        # 현재 시퀀스 + 예측 글자를 현재 시퀀스로 변경\n",
        "        seed_text = seed_text + char\n",
        "\n",
        "        # 예측 글자를 문장에 저장\n",
        "        sentence = sentence + char\n",
        "\n",
        "    sentence = init_text + sentence\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5kIEaBq2833",
        "outputId": "2ca28b55-d865-4ee2-b113-1ec86c3fea5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I get on with life as a programmer, I like to use words about beer. But when I sto t to ra\n"
          ]
        }
      ],
      "source": [
        "print(sentence_generation(model, char_to_index, 10, 'I get on w', 80))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Char RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
