{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SNksHfLkf5l6"
      },
      "source": [
        "이 코드는 2021년 12월 14일에 tensorflow 2.7 버전으로 마지막으로 테스트 되었습니다.  \n",
        "\n",
        "이 코드는 위키독스 '딥 러닝을 이용한 자연어 처리 입문'의 seq2seq 튜토리얼입니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uoEgIOcFhccg"
      },
      "source": [
        "링크 : https://wikidocs.net/24996"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IfN4gZMKh0yc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.9.3'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import urllib3\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6y9XPRYCiagc"
      },
      "outputs": [],
      "source": [
        "http = urllib3.PoolManager()\n",
        "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
        "filename = 'fra-eng.zip'\n",
        "path = os.getcwd()\n",
        "zipfilename = os.path.join(path, filename)\n",
        "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
        "    shutil.copyfileobj(r, out_file)\n",
        "\n",
        "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdt-athJibxC",
        "outputId": "00cfd8b9-1232-442e-beb1-eec489b74da1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 샘플의 개수 : 217975\n"
          ]
        }
      ],
      "source": [
        "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
        "del lines['lic']\n",
        "print('전체 샘플의 개수 :',len(lines))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "OJWHLXyricub",
        "outputId": "b4d78f5e-2cfa-434a-f0d4-b1c2e126658f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17744</th>\n",
              "      <td>We need a medic.</td>\n",
              "      <td>Il nous faut un médecin.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3533</th>\n",
              "      <td>I'm prudent.</td>\n",
              "      <td>Je suis prudent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43801</th>\n",
              "      <td>Who did you go with?</td>\n",
              "      <td>Avec qui êtes-vous allé ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9088</th>\n",
              "      <td>They're crazy.</td>\n",
              "      <td>Elles sont folles.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59859</th>\n",
              "      <td>You forgot the spoons.</td>\n",
              "      <td>Tu as oublié les cuillères.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46871</th>\n",
              "      <td>I know it's not fair.</td>\n",
              "      <td>Je sais que ce n'est pas juste.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11715</th>\n",
              "      <td>Is Tom working?</td>\n",
              "      <td>Est-ce que Tom travaille ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7660</th>\n",
              "      <td>I have doubts.</td>\n",
              "      <td>J'ai des doutes.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35008</th>\n",
              "      <td>There is some wind.</td>\n",
              "      <td>Il y a un peu de vent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37153</th>\n",
              "      <td>You'd better leave.</td>\n",
              "      <td>Mieux vaut que vous partiez.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          src                              tar\n",
              "17744        We need a medic.         Il nous faut un médecin.\n",
              "3533             I'm prudent.                 Je suis prudent.\n",
              "43801    Who did you go with?        Avec qui êtes-vous allé ?\n",
              "9088           They're crazy.               Elles sont folles.\n",
              "59859  You forgot the spoons.      Tu as oublié les cuillères.\n",
              "46871   I know it's not fair.  Je sais que ce n'est pas juste.\n",
              "11715         Is Tom working?       Est-ce que Tom travaille ?\n",
              "7660           I have doubts.                 J'ai des doutes.\n",
              "35008     There is some wind.           Il y a un peu de vent.\n",
              "37153     You'd better leave.     Mieux vaut que vous partiez."
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines = lines.loc[:, 'src':'tar']\n",
        "lines = lines[0:60000] # 6만개만 저장\n",
        "lines.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "HC80PHpgidj6",
        "outputId": "793e8f2c-d0a3-4db0-c1b2-0f33a96762c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>tar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>57709</th>\n",
              "      <td>The moon has come out.</td>\n",
              "      <td>\\t La lune est apparue. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51488</th>\n",
              "      <td>Where are the horses?</td>\n",
              "      <td>\\t Où sont les chevaux ? \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1811</th>\n",
              "      <td>Do I stink?</td>\n",
              "      <td>\\t Est-ce que je pue ? \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58725</th>\n",
              "      <td>Tom's cat is adorable.</td>\n",
              "      <td>\\t Le chat de Tom est adorable. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6093</th>\n",
              "      <td>Thanks a lot.</td>\n",
              "      <td>\\t Merci bien. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26092</th>\n",
              "      <td>I like your style.</td>\n",
              "      <td>\\t J'aime votre style. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40967</th>\n",
              "      <td>It's useless to try.</td>\n",
              "      <td>\\t C'est inutile d'essayer. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54072</th>\n",
              "      <td>I acted on his advice.</td>\n",
              "      <td>\\t J'ai agi conformément à son conseil. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50112</th>\n",
              "      <td>This is a great idea.</td>\n",
              "      <td>\\t C'est une idée merveilleuse. \\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5472</th>\n",
              "      <td>I'm not dead.</td>\n",
              "      <td>\\t Je ne suis pas morte. \\n</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          src                                         tar\n",
              "57709  The moon has come out.                  \\t La lune est apparue. \\n\n",
              "51488   Where are the horses?                 \\t Où sont les chevaux ? \\n\n",
              "1811              Do I stink?                   \\t Est-ce que je pue ? \\n\n",
              "58725  Tom's cat is adorable.          \\t Le chat de Tom est adorable. \\n\n",
              "6093            Thanks a lot.                           \\t Merci bien. \\n\n",
              "26092      I like your style.                   \\t J'aime votre style. \\n\n",
              "40967    It's useless to try.              \\t C'est inutile d'essayer. \\n\n",
              "54072  I acted on his advice.  \\t J'ai agi conformément à son conseil. \\n\n",
              "50112   This is a great idea.          \\t C'est une idée merveilleuse. \\n\n",
              "5472            I'm not dead.                 \\t Je ne suis pas morte. \\n"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines.tar = lines.tar.apply(lambda x : '\\t ' + x + ' \\n')\n",
        "lines.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "p5VyYPo3iedy"
      },
      "outputs": [],
      "source": [
        "# 글자 집합 구축\n",
        "src_vocab = set()\n",
        "for line in lines.src:  # 1줄씩 읽음\n",
        "    for char in line:   # 1개의 글자씩 읽음\n",
        "        src_vocab.add(char)\n",
        "\n",
        "tar_vocab = set()\n",
        "for line in lines.tar:\n",
        "    for char in line:\n",
        "        tar_vocab.add(char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfg9pOB7ifgK",
        "outputId": "fd5eca24-ce9e-425f-ce37-37886f7860bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source 문장의 char 집합 : 80\n",
            "target 문장의 char 집합 : 103\n"
          ]
        }
      ],
      "source": [
        "src_vocab_size = len(src_vocab) + 1\n",
        "tar_vocab_size = len(tar_vocab) + 1\n",
        "print('source 문장의 char 집합 :', src_vocab_size)\n",
        "print('target 문장의 char 집합 :', tar_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJJolPvRignC",
        "outputId": "52d760f9-1003-42be-8b0b-4cc769860fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "['T', 'U', 'V', 'W', 'X', 'Y', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x']\n"
          ]
        }
      ],
      "source": [
        "src_vocab = sorted(list(src_vocab))\n",
        "tar_vocab = sorted(list(tar_vocab))\n",
        "print(src_vocab[45:75])\n",
        "print(tar_vocab[45:75])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQusbB6GiiH7",
        "outputId": "35d3056b-5e38-4097-ecb6-f1927d56b9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, '°': 76, 'é': 77, '’': 78, '€': 79}\n",
            "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'a': 52, 'b': 53, 'c': 54, 'd': 55, 'e': 56, 'f': 57, 'g': 58, 'h': 59, 'i': 60, 'j': 61, 'k': 62, 'l': 63, 'm': 64, 'n': 65, 'o': 66, 'p': 67, 'q': 68, 'r': 69, 's': 70, 't': 71, 'u': 72, 'v': 73, 'w': 74, 'x': 75, 'y': 76, 'z': 77, '\\xa0': 78, '«': 79, '»': 80, 'À': 81, 'Ç': 82, 'É': 83, 'Ê': 84, 'Ô': 85, 'à': 86, 'â': 87, 'ç': 88, 'è': 89, 'é': 90, 'ê': 91, 'ë': 92, 'î': 93, 'ï': 94, 'ô': 95, 'ù': 96, 'û': 97, 'œ': 98, '\\u2009': 99, '‘': 100, '’': 101, '\\u202f': 102}\n"
          ]
        }
      ],
      "source": [
        "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
        "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
        "print(src_to_index)\n",
        "print(tar_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    Go.\n",
              "1    Go.\n",
              "2    Go.\n",
              "3    Go.\n",
              "4    Hi.\n",
              "Name: src, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines.src[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeHzq5rNijWS",
        "outputId": "186f24f9-4dd8-4633-f031-ee7ebe7dec28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source 문장의 정수 인코딩 : [[30, 64, 10], [30, 64, 10], [30, 64, 10], [30, 64, 10], [31, 58, 10]]\n"
          ]
        }
      ],
      "source": [
        "encoder_input = []\n",
        "\n",
        "# 1개의 문장\n",
        "for line in lines.src:\n",
        "    encoded_line = []\n",
        "    # 각 줄에서 1개의 char\n",
        "    for char in line:\n",
        "        # 각 char을 정수로 변환\n",
        "        encoded_line.append(src_to_index[char])\n",
        "    encoder_input.append(encoded_line)\n",
        "\n",
        "print('source 문장의 정수 인코딩 :', encoder_input[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0          \\t Va ! \\n\n",
              "1       \\t Marche. \\n\n",
              "2    \\t En route ! \\n\n",
              "3       \\t Bouge ! \\n\n",
              "4       \\t Salut ! \\n\n",
              "Name: tar, dtype: object"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lines.tar[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MFjg8quikj6",
        "outputId": "31bd4e3f-1e8d-4ff5-d2c5-64929455cb44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target 문장의 정수 인코딩 : [[1, 3, 48, 52, 3, 4, 3, 2], [1, 3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [1, 3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [1, 3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [1, 3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
          ]
        }
      ],
      "source": [
        "decoder_input = []\n",
        "\n",
        "for line in lines.tar:\n",
        "    encoded_line = []\n",
        "    for char in line:\n",
        "        encoded_line.append(tar_to_index[char])\n",
        "    decoder_input.append(encoded_line)\n",
        "\n",
        "print('target 문장의 정수 인코딩 :', decoder_input[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6RS186uilqK",
        "outputId": "389b4b8f-45dc-4016-cb11-5f85d0df3738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "target 문장 레이블의 정수 인코딩 : [[3, 48, 52, 3, 4, 3, 2], [3, 39, 52, 69, 54, 59, 56, 14, 3, 2], [3, 31, 65, 3, 69, 66, 72, 71, 56, 3, 4, 3, 2], [3, 28, 66, 72, 58, 56, 3, 4, 3, 2], [3, 45, 52, 63, 72, 71, 3, 4, 3, 2]]\n"
          ]
        }
      ],
      "source": [
        "decoder_target = []\n",
        "\n",
        "for line in lines.tar:\n",
        "    timestep = 0\n",
        "    encoded_line = []\n",
        "    for char in line:\n",
        "        if timestep > 0:\n",
        "            encoded_line.append(tar_to_index[char])\n",
        "        timestep = timestep + 1\n",
        "    decoder_target.append(encoded_line)\n",
        "    \n",
        "print('target 문장 레이블의 정수 인코딩 :', decoder_target[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zlq71juimoy",
        "outputId": "92a28855-af9c-44f7-94ce-3ed0c067cb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source 문장의 최대 길이 : 22\n",
            "target 문장의 최대 길이 : 76\n"
          ]
        }
      ],
      "source": [
        "max_src_len = max([len(line) for line in lines.src])\n",
        "max_tar_len = max([len(line) for line in lines.tar])\n",
        "print('source 문장의 최대 길이 :', max_src_len)\n",
        "print('target 문장의 최대 길이 :', max_tar_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FcnFgd9Zinsa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of encoder_input :  (60000, 22)\n",
            "shape of decoder_input :  (60000, 76)\n",
            "shape of decoder_target :  (60000, 76)\n"
          ]
        }
      ],
      "source": [
        "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n",
        "\n",
        "print('shape of encoder_input : ', encoder_input.shape)\n",
        "print('shape of decoder_input : ', decoder_input.shape)\n",
        "print('shape of decoder_target : ', decoder_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "EcXSOrECio3C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape of encoder_input :  (60000, 22, 80)\n",
            "shape of decoder_input :  (60000, 76, 103)\n",
            "shape of decoder_target :  (60000, 76, 103)\n"
          ]
        }
      ],
      "source": [
        "encoder_input = to_categorical(encoder_input)\n",
        "decoder_input = to_categorical(decoder_input)\n",
        "decoder_target = to_categorical(decoder_target)\n",
        "\n",
        "print('shape of encoder_input : ', encoder_input.shape)\n",
        "print('shape of decoder_input : ', decoder_input.shape)\n",
        "print('shape of decoder_target : ', decoder_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0GqUpQ_Rippq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "C9blN5WziqtS"
      },
      "outputs": [],
      "source": [
        "# src_vocab_size=80\n",
        "# shape of encoder_inputs : (None, None, 80)\n",
        "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
        "encoder_lstm = LSTM(units=256, return_state=True)\n",
        "\n",
        "# encoder_outputs은 여기서는 불필요\n",
        "# shape of encoder_outputs : (None, 256)\n",
        "# shape of state_h : (None, 256)\n",
        "# shape of state_c : (None, 256)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "\n",
        "# LSTM은 바닐라 RNN과는 달리 상태가 두 개. 은닉 상태와 셀 상태.\n",
        "encoder_states = [state_h, state_c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "keZfw7g2irvK"
      },
      "outputs": [],
      "source": [
        "# tar_vocab_size=103\n",
        "# shape of decoder_inputs : (None, None, 103)\n",
        "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
        "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
        "\n",
        "# 디코더에게 인코더의 은닉 상태, 셀 상태를 전달.\n",
        "# shape of decoder_outputs : (None, None, 256)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "\n",
        "# shape of decoder_outputs : (None, None, 103)\n",
        "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q82rA2Ixisor",
        "outputId": "83781e04-9947-479a-e690-e610299f7695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "750/750 [==============================] - 17s 20ms/step - loss: 0.7379 - val_loss: 0.6436\n",
            "Epoch 2/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.4455 - val_loss: 0.5092\n",
            "Epoch 3/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.3706 - val_loss: 0.4474\n",
            "Epoch 4/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.3287 - val_loss: 0.4096\n",
            "Epoch 5/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.3008 - val_loss: 0.3888\n",
            "Epoch 6/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.2806 - val_loss: 0.3734\n",
            "Epoch 7/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.2648 - val_loss: 0.3616\n",
            "Epoch 8/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.2521 - val_loss: 0.3536\n",
            "Epoch 9/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.2415 - val_loss: 0.3464\n",
            "Epoch 10/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.2323 - val_loss: 0.3410\n",
            "Epoch 11/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.2243 - val_loss: 0.3389\n",
            "Epoch 12/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.2170 - val_loss: 0.3362\n",
            "Epoch 13/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.2105 - val_loss: 0.3338\n",
            "Epoch 14/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.2047 - val_loss: 0.3337\n",
            "Epoch 15/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1991 - val_loss: 0.3333\n",
            "Epoch 16/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1943 - val_loss: 0.3325\n",
            "Epoch 17/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1895 - val_loss: 0.3339\n",
            "Epoch 18/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1852 - val_loss: 0.3338\n",
            "Epoch 19/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1811 - val_loss: 0.3335\n",
            "Epoch 20/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1772 - val_loss: 0.3369\n",
            "Epoch 21/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1736 - val_loss: 0.3371\n",
            "Epoch 22/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1703 - val_loss: 0.3402\n",
            "Epoch 23/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1670 - val_loss: 0.3401\n",
            "Epoch 24/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1639 - val_loss: 0.3438\n",
            "Epoch 25/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1610 - val_loss: 0.3444\n",
            "Epoch 26/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1582 - val_loss: 0.3450\n",
            "Epoch 27/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1556 - val_loss: 0.3484\n",
            "Epoch 28/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1531 - val_loss: 0.3539\n",
            "Epoch 29/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1505 - val_loss: 0.3543\n",
            "Epoch 30/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1484 - val_loss: 0.3562\n",
            "Epoch 31/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1460 - val_loss: 0.3598\n",
            "Epoch 32/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1440 - val_loss: 0.3605\n",
            "Epoch 33/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1418 - val_loss: 0.3670\n",
            "Epoch 34/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1399 - val_loss: 0.3673\n",
            "Epoch 35/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1381 - val_loss: 0.3696\n",
            "Epoch 36/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1362 - val_loss: 0.3726\n",
            "Epoch 37/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1344 - val_loss: 0.3760\n",
            "Epoch 38/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1328 - val_loss: 0.3788\n",
            "Epoch 39/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1311 - val_loss: 0.3806\n",
            "Epoch 40/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1295 - val_loss: 0.3833\n",
            "Epoch 41/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1280 - val_loss: 0.3874\n",
            "Epoch 42/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1265 - val_loss: 0.3876\n",
            "Epoch 43/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1252 - val_loss: 0.3908\n",
            "Epoch 44/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1236 - val_loss: 0.3935\n",
            "Epoch 45/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1224 - val_loss: 0.3955\n",
            "Epoch 46/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1211 - val_loss: 0.3987\n",
            "Epoch 47/50\n",
            "750/750 [==============================] - 15s 19ms/step - loss: 0.1198 - val_loss: 0.4011\n",
            "Epoch 48/50\n",
            "750/750 [==============================] - 16s 21ms/step - loss: 0.1186 - val_loss: 0.4044\n",
            "Epoch 49/50\n",
            "750/750 [==============================] - 15s 21ms/step - loss: 0.1175 - val_loss: 0.4063\n",
            "Epoch 50/50\n",
            "750/750 [==============================] - 15s 20ms/step - loss: 0.1163 - val_loss: 0.4115\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ad2972df0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"categorical_crossentropy\"\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    x=[encoder_input, decoder_input],\n",
        "    y=decoder_target,\n",
        "    batch_size=64,\n",
        "    epochs=50,\n",
        "    validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PvAzKE5divNi"
      },
      "outputs": [],
      "source": [
        "encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OssDkukziwpK",
        "outputId": "ae24f70f-035d-4f6b-acd4-897e6aede90e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, 80)]        0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 256),             345088    \n",
            "                              (None, 256),                       \n",
            "                              (None, 256)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 345,088\n",
            "Trainable params: 345,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rnBYHCbOizSi"
      },
      "outputs": [],
      "source": [
        "# 이전 시점의 상태들을 저장하는 텐서\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
        "# 뒤의 함수 decode_sequence()에 동작을 구현 예정\n",
        "# shape of decoder_inputs : (None, None, 103)\n",
        "# shape of decoder_outputs : (None, None, 256)\n",
        "# shape of state_h : (None, 256)\n",
        "# shape of state_c : (None, 256)\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "\n",
        "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태를 버리지 않음.\n",
        "# shape of decoder_outputs : (None, None, 103)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    inputs=[decoder_inputs] + decoder_states_inputs,\n",
        "    outputs=[decoder_outputs] + decoder_states\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None, 103)]  0           []                               \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 256)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  368640      ['input_2[0][0]',                \n",
            "                                 (None, 256),                     'input_3[0][0]',                \n",
            "                                 (None, 256)]                     'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 103)    26471       ['lstm_1[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 395,111\n",
            "Trainable params: 395,111\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "decoder_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tYMVxO_MgT-6"
      },
      "outputs": [],
      "source": [
        "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
        "index_to_tar = dict((i, char) for char, i in tar_to_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w9WAlasJgVbK"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # 입력으로부터 인코더의 상태를 얻음\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
        "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    # stop_condition이 True가 될 때까지 루프 반복\n",
        "    while not stop_condition:\n",
        "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # 예측 결과를 문자로 변환\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = index_to_tar[sampled_token_index]\n",
        "\n",
        "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
        "        if (sampled_char == '\\n' or\n",
        "            len(decoded_sentence) > max_tar_len):\n",
        "            stop_condition = True\n",
        "\n",
        "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
        "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGLAMcZSgWaq",
        "outputId": "ea6c2aad-dedc-46ef-b063-c14dd19d44ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 187ms/step\n",
            "1/1 [==============================] - 0s 171ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Go.\n",
            "정답 문장: Bouge ! \n",
            "번역 문장: Décampe ! \n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 11ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Hello!\n",
            "정답 문장: Bonjour ! \n",
            "번역 문장: Bonjour ! \n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Got it!\n",
            "정답 문장: Compris ! \n",
            "번역 문장: Compris ! \n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Goodbye.\n",
            "정답 문장: Au revoir. \n",
            "번역 문장: Casse-toi. \n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 9ms/step\n",
            "1/1 [==============================] - 0s 10ms/step\n",
            "-----------------------------------\n",
            "입력 문장: Hands off.\n",
            "정답 문장: Pas touche ! \n",
            "번역 문장: Va ! \n"
          ]
        }
      ],
      "source": [
        "for seq_index in [3, 50, 100, 300, 1001]: # 입력 문장의 인덱스\n",
        "    input_seq = encoder_input[seq_index:seq_index+1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    \n",
        "    print(35 * \"-\")\n",
        "    print('입력 문장:', lines.src[seq_index])\n",
        "    print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
        "    print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "char-level seq2seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
